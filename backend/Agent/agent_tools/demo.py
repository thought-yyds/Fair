import os
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
import logging
import numpy as np
from concurrent.futures import ThreadPoolExecutor  # æ¢å¤å¹¶è¡Œä¾èµ–

# å‘é‡å­˜å‚¨ & æ£€ç´¢
import faiss
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain_huggingface import HuggingFaceEmbeddings
from rank_bm25 import BM25Okapi
import jieba
from langchain.schema import Document

# æ–¹èˆŸAPI
from config.settings import get_settings
from API import LLMClient

# BERT æ¨¡å‹ä¾èµ–ï¼ˆæ¿€æ´»é¢„æµ‹åŠŸèƒ½ï¼‰
import torch
from transformers import BertTokenizer, BertForSequenceClassification

# ==============================================================================
# é…ç½®åŒºï¼ˆé€‚é…ç¯å¢ƒ+è°ƒè¯•å‹å¥½ï¼‰
# ==============================================================================
MODEL_CACHE_DIR = "/home/grp/disk1/Huggface"
os.environ["TRANSFORMERS_CACHE"] = MODEL_CACHE_DIR
os.makedirs(MODEL_CACHE_DIR, exist_ok=True)

# è½»é‡ Embedding æ¨¡å‹
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
EMBEDDING_DEVICE = "cuda:2" if torch.cuda.is_available() else "cpu"

# BERT æ¨¡å‹è·¯å¾„ï¼ˆæ¿€æ´»é¢„æµ‹ï¼‰
BERT_MODEL_PATH = "/home/grp/disk1/Fair_detection/encapsulation/two_stage_model.pt"
BERT_TOKENIZER_PATH = "/home/grp/disk1/Agent/tokenizer"
BERT_CONFIDENCE_THRESHOLD = 0.05  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šç»“æœï¼ˆè°ƒè¯•ç”¨ï¼‰

# è¿è§„è§„åˆ™æ˜ å°„ï¼ˆç”¨äºBERTç»“æœè§£æï¼‰
RULE_MAPPING = {
    0: "ä¸è¿è§„",
    1: "è®¾ç½®æ˜æ˜¾ä¸å¿…è¦æˆ–è€…è¶…å‡ºå®é™…éœ€è¦çš„å‡†å…¥å’Œé€€å‡ºæ¡ä»¶ï¼Œæ’æ–¥æˆ–è€…é™åˆ¶ç»è¥è€…å‚ä¸å¸‚åœºç«äº‰",
    2: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå¯¹ä¸åŒæ‰€æœ‰åˆ¶ã€åœ°åŒºã€ç»„ç»‡å½¢å¼çš„ç»è¥è€…å®æ–½ä¸åˆç†çš„å·®åˆ«åŒ–å¾…é‡ï¼Œè®¾ç½®ä¸å¹³ç­‰çš„å¸‚åœºå‡†å…¥å’Œé€€å‡ºæ¡ä»¶",
    3: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œä»¥å¤‡æ¡ˆã€ç™»è®°ã€æ³¨å†Œã€ç›®å½•ã€å¹´æ£€ã€å¹´æŠ¥ã€ç›‘åˆ¶ã€è®¤å®šã€è®¤è¯ã€è®¤å¯ã€æ£€éªŒã€ç›‘æµ‹ã€å®¡å®šã€æŒ‡å®šã€é…å·ã€å¤æ£€ã€å¤å®¡ã€æ¢è¯ã€è¦æ±‚è®¾ç«‹åˆ†æ”¯æœºæ„ä»¥åŠå…¶ä»–ä»»ä½•å½¢å¼ï¼Œè®¾å®šæˆ–è€…å˜ç›¸è®¾å®šå¸‚åœºå‡†å…¥éšœç¢",
    4: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå¯¹ä¼ä¸šæ³¨é”€ã€ç ´äº§ã€æŒ‚ç‰Œè½¬è®©ã€æ¬è¿è½¬ç§»ç­‰è®¾å®šæˆ–è€…å˜ç›¸è®¾å®šå¸‚åœºé€€å‡ºéšœç¢",
    5: "ä»¥è¡Œæ”¿è®¸å¯ã€è¡Œæ”¿æ£€æŸ¥ã€è¡Œæ”¿å¤„ç½šã€è¡Œæ”¿å¼ºåˆ¶ç­‰æ–¹å¼ï¼Œå¼ºåˆ¶æˆ–è€…å˜ç›¸å¼ºåˆ¶ä¼ä¸šè½¬è®©æŠ€æœ¯ï¼Œè®¾å®šæˆ–è€…å˜ç›¸è®¾å®šå¸‚åœºå‡†å…¥å’Œé€€å‡ºéšœç¢",
    6: "åœ¨ä¸€èˆ¬ç«äº‰æ€§é¢†åŸŸå®æ–½ç‰¹è®¸ç»è¥æˆ–è€…ä»¥ç‰¹è®¸ç»è¥ä¸ºåå¢è®¾è¡Œæ”¿è®¸å¯",
    7: "æœªæ˜ç¡®ç‰¹è®¸ç»è¥æƒæœŸé™æˆ–è€…æœªç»æ³•å®šç¨‹åºå»¶é•¿ç‰¹è®¸ç»è¥æƒæœŸé™",
    8: "æœªä¾æ³•é‡‡å–æ‹›æ ‡ã€ç«äº‰æ€§è°ˆåˆ¤ç­‰ç«äº‰æ–¹å¼ï¼Œç›´æ¥å°†ç‰¹è®¸ç»è¥æƒæˆäºˆç‰¹å®šç»è¥è€…",
    9: "è®¾ç½®æ­§è§†æ€§æ¡ä»¶ï¼Œä½¿ç»è¥è€…æ— æ³•å…¬å¹³å‚ä¸ç‰¹è®¸ç»è¥æƒç«äº‰",
    10: "ä»¥æ˜ç¡®è¦æ±‚ã€æš—ç¤ºã€æ‹’ç»æˆ–è€…æ‹–å»¶è¡Œæ”¿å®¡æ‰¹ã€é‡å¤æ£€æŸ¥ã€ä¸äºˆæ¥å…¥å¹³å°æˆ–è€…ç½‘ç»œã€è¿æ³•è¿è§„ç»™äºˆå¥–åŠ±è¡¥è´´ç­‰æ–¹å¼ï¼Œé™å®šæˆ–è€…å˜ç›¸é™å®šç»è¥ã€è´­ä¹°ã€ä½¿ç”¨ç‰¹å®šç»è¥è€…æä¾›çš„å•†å“å’ŒæœåŠ¡",
    11: "åœ¨æ‹›æ ‡æŠ•æ ‡ã€æ”¿åºœé‡‡è´­ä¸­é™å®šæŠ•æ ‡äººæ‰€åœ¨åœ°ã€æ‰€æœ‰åˆ¶å½¢å¼ã€ç»„ç»‡å½¢å¼ï¼Œæˆ–è€…è®¾å®šå…¶ä»–ä¸åˆç†çš„æ¡ä»¶æ’æ–¥æˆ–è€…é™åˆ¶ç»è¥è€…å‚ä¸æ‹›æ ‡æŠ•æ ‡ã€æ”¿åºœé‡‡è´­æ´»åŠ¨",
    12: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œé€šè¿‡è®¾ç½®ä¸åˆç†çš„é¡¹ç›®åº“ã€åå½•åº“ã€å¤‡é€‰åº“ã€èµ„æ ¼åº“ç­‰æ¡ä»¶ï¼Œæ’æ–¥æˆ–é™åˆ¶æ½œåœ¨ç»è¥è€…æä¾›å•†å“å’ŒæœåŠ¡",
    13: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå¢è®¾è¡Œæ”¿å®¡æ‰¹äº‹é¡¹ï¼Œå¢åŠ è¡Œæ”¿å®¡æ‰¹ç¯èŠ‚ã€æ¡ä»¶å’Œç¨‹åº",
    14: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œè®¾ç½®å…·æœ‰è¡Œæ”¿å®¡æ‰¹æ€§è´¨çš„å‰ç½®æ€§å¤‡æ¡ˆç¨‹åº",
    15: "ä¸å¾—å¯¹å¸‚åœºå‡†å…¥è´Ÿé¢æ¸…å•ä»¥å¤–çš„è¡Œä¸šã€é¢†åŸŸã€ä¸šåŠ¡ç­‰è®¾ç½®å®¡æ‰¹ç¨‹åºï¼Œä¸»è¦æŒ‡æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œé‡‡å–ç¦æ­¢è¿›å…¥ã€é™åˆ¶å¸‚åœºä¸»ä½“èµ„è´¨ã€é™åˆ¶è‚¡æƒæ¯”ä¾‹ã€é™åˆ¶ç»è¥èŒƒå›´å’Œå•†ä¸šæ¨¡å¼ç­‰æ–¹å¼ï¼Œé™åˆ¶æˆ–è€…å˜ç›¸é™åˆ¶å¸‚åœºå‡†å…¥",
    16: "åˆ¶å®šæ”¿åºœå®šä»·æˆ–è€…æ”¿åºœæŒ‡å¯¼ä»·æ—¶ï¼Œå¯¹å¤–åœ°å’Œè¿›å£åŒç±»å•†å“ã€æœåŠ¡åˆ¶å®šæ­§è§†æ€§ä»·æ ¼",
    17: "å¯¹ç›¸å…³å•†å“ã€æœåŠ¡è¿›è¡Œè¡¥è´´æ—¶ï¼Œå¯¹å¤–åœ°åŒç±»å•†å“ã€æœåŠ¡ï¼Œå›½é™…ç»è´¸åå®šå…è®¸å¤–çš„è¿›å£åŒç±»å•†å“ä»¥åŠæˆ‘å›½ä½œå‡ºå›½é™…æ‰¿è¯ºçš„è¿›å£åŒç±»æœåŠ¡ä¸äºˆè¡¥è´´æˆ–è€…ç»™äºˆè¾ƒä½è¡¥è´´",
    18: "å¯¹å¤–åœ°å•†å“ã€æœåŠ¡è§„å®šä¸æœ¬åœ°åŒç±»å•†å“ã€æœåŠ¡ä¸åŒçš„æŠ€æœ¯è¦æ±‚ã€æ£€éªŒæ ‡å‡†ï¼Œæˆ–è€…é‡‡å–é‡å¤æ£€éªŒã€é‡å¤è®¤è¯ç­‰æ­§è§†æ€§æŠ€æœ¯æªæ–½",
    19: "å¯¹è¿›å£å•†å“è§„å®šä¸æœ¬åœ°åŒç±»å•†å“ä¸åŒçš„æŠ€æœ¯è¦æ±‚ã€æ£€éªŒæ ‡å‡†ï¼Œæˆ–è€…é‡‡å–é‡å¤æ£€éªŒã€é‡å¤è®¤è¯ç­‰æ­§è§†æ€§æŠ€æœ¯æªæ–½",
    20: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå¯¹è¿›å£æœåŠ¡è§„å®šä¸æœ¬åœ°åŒç±»æœåŠ¡ä¸åŒçš„æŠ€æœ¯è¦æ±‚ã€æ£€éªŒæ ‡å‡†ï¼Œæˆ–è€…é‡‡å–é‡å¤æ£€éªŒã€é‡å¤è®¤è¯ç­‰æ­§è§†æ€§æŠ€æœ¯æªæ–½",
    21: "è®¾ç½®ä¸“é—¨é’ˆå¯¹å¤–åœ°å’Œè¿›å£å•†å“ã€æœåŠ¡çš„ä¸“è¥ã€ä¸“å–ã€å®¡æ‰¹ã€è®¸å¯ã€å¤‡æ¡ˆï¼Œæˆ–è€…è§„å®šä¸åŒçš„æ¡ä»¶ã€ç¨‹åºå’ŒæœŸé™ç­‰",
    22: "åœ¨é“è·¯ã€è½¦ç«™ã€æ¸¯å£ã€èˆªç©ºæ¸¯æˆ–è€…æœ¬è¡Œæ”¿åŒºåŸŸè¾¹ç•Œè®¾ç½®å…³å¡ï¼Œé˜»ç¢å¤–åœ°å’Œè¿›å£å•†å“ã€æœåŠ¡è¿›å…¥æœ¬åœ°å¸‚åœºæˆ–è€…æœ¬åœ°å•†å“è¿å‡ºå’ŒæœåŠ¡è¾“å‡º",
    23: "é€šè¿‡è½¯ä»¶æˆ–è€…äº’è”ç½‘è®¾ç½®å±è”½ä»¥åŠé‡‡å–å…¶ä»–æ‰‹æ®µï¼Œé˜»ç¢å¤–åœ°å’Œè¿›å£å•†å“ã€æœåŠ¡è¿›å…¥æœ¬åœ°å¸‚åœºæˆ–è€…æœ¬åœ°å•†å“è¿å‡ºå’ŒæœåŠ¡è¾“å‡º",
    24: "ä¸ä¾æ³•åŠæ—¶ã€æœ‰æ•ˆã€å®Œæ•´åœ°å‘å¸ƒæ‹›æ ‡ä¿¡æ¯",
    25: "ç›´æ¥è§„å®šå¤–åœ°ç»è¥è€…ä¸èƒ½å‚ä¸æœ¬åœ°ç‰¹å®šçš„æ‹›æ ‡æŠ•æ ‡æ´»åŠ¨",
    26: "å¯¹å¤–åœ°ç»è¥è€…è®¾å®šæ­§è§†æ€§çš„èµ„è´¨èµ„æ ¼è¦æ±‚æˆ–è€…è¯„æ ‡è¯„å®¡æ ‡å‡†",
    27: "å°†ç»è¥è€…åœ¨æœ¬åœ°åŒºçš„ä¸šç»©ã€æ‰€è·å¾—çš„å¥–é¡¹è£èª‰ä½œä¸ºæŠ•æ ‡æ¡ä»¶ã€åŠ åˆ†æ¡ä»¶ã€ä¸­æ ‡æ¡ä»¶æˆ–è€…ç”¨äºè¯„ä»·ä¼ä¸šä¿¡ç”¨ç­‰çº§ï¼Œé™åˆ¶æˆ–è€…å˜ç›¸é™åˆ¶å¤–åœ°ç»è¥è€…å‚åŠ æœ¬åœ°æ‹›æ ‡æŠ•æ ‡æ´»åŠ¨",
    28: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œè¦æ±‚ç»è¥è€…åœ¨æœ¬åœ°æ³¨å†Œè®¾ç«‹åˆ†æ”¯æœºæ„ï¼Œåœ¨æœ¬åœ°æ‹¥æœ‰ä¸€å®šåŠå…¬é¢ç§¯ï¼Œåœ¨æœ¬åœ°ç¼´çº³ç¤¾ä¼šä¿é™©ç­‰ï¼Œé™åˆ¶æˆ–è€…å˜ç›¸é™åˆ¶å¤–åœ°ç»è¥è€…å‚åŠ æœ¬åœ°æ‹›æ ‡æŠ•æ ‡æ´»åŠ¨",
    29: "é€šè¿‡è®¾å®šä¸æ‹›æ ‡é¡¹ç›®çš„å…·ä½“ç‰¹ç‚¹å’Œå®é™…éœ€è¦ä¸ç›¸é€‚åº”æˆ–è€…ä¸åˆåŒå±¥è¡Œæ— å…³çš„èµ„æ ¼ã€æŠ€æœ¯å’Œå•†åŠ¡æ¡ä»¶ï¼Œé™åˆ¶æˆ–è€…å˜ç›¸é™åˆ¶å¤–åœ°ç»è¥è€…å‚åŠ æœ¬åœ°æ‹›æ ‡æŠ•æ ‡æ´»åŠ¨",
    30: "ç›´æ¥æ‹’ç»å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°æŠ•èµ„æˆ–è€…è®¾ç«‹åˆ†æ”¯æœºæ„",
    31: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå¯¹å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°æŠ•èµ„çš„è§„æ¨¡ã€æ–¹å¼ä»¥åŠè®¾ç«‹åˆ†æ”¯æœºæ„çš„åœ°å€ã€æ¨¡å¼ç­‰è¿›è¡Œé™åˆ¶",
    32: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œç›´æ¥å¼ºåˆ¶å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°æŠ•èµ„æˆ–è€…è®¾ç«‹åˆ†æ”¯æœºæ„",
    33: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå°†åœ¨æœ¬åœ°æŠ•èµ„æˆ–è€…è®¾ç«‹åˆ†æ”¯æœºæ„ä½œä¸ºå‚ä¸æœ¬åœ°æ‹›æ ‡æŠ•æ ‡ã€äº«å—è¡¥è´´å’Œä¼˜æƒ æ”¿ç­–ç­‰çš„å¿…è¦æ¡ä»¶ï¼Œå˜ç›¸å¼ºåˆ¶å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°æŠ•èµ„æˆ–è€…è®¾ç«‹åˆ†æ”¯æœºæ„",
    34: "å¯¹å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°çš„æŠ•èµ„ä¸ç»™äºˆä¸æœ¬åœ°ç»è¥è€…åŒç­‰çš„æ”¿ç­–å¾…é‡",
    35: "å¯¹å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°è®¾ç«‹çš„åˆ†æ”¯æœºæ„åœ¨ç»è¥è§„æ¨¡ã€ç»è¥æ–¹å¼ã€ç¨è´¹ç¼´çº³ç­‰æ–¹é¢è§„å®šä¸æœ¬åœ°ç»è¥è€…ä¸åŒçš„è¦æ±‚",
    36: "åœ¨èŠ‚èƒ½ç¯ä¿ã€å®‰å…¨ç”Ÿäº§ã€å¥åº·å«ç”Ÿã€å·¥ç¨‹è´¨é‡ã€å¸‚åœºç›‘ç®¡ç­‰æ–¹é¢ï¼Œå¯¹å¤–åœ°ç»è¥è€…åœ¨æœ¬åœ°è®¾ç«‹çš„åˆ†æ”¯æœºæ„è§„å®šæ­§è§†æ€§ç›‘ç®¡æ ‡å‡†å’Œè¦æ±‚",
    37: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œç»™äºˆç‰¹å®šç»è¥è€…è´¢æ”¿å¥–åŠ±å’Œè¡¥è´´",
    38: "æ²¡æœ‰ä¸“é—¨çš„ç¨æ”¶æ³•å¾‹ã€æ³•è§„å’Œå›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œç»™äºˆç‰¹å®šç»è¥è€…ç¨æ”¶ä¼˜æƒ æ”¿ç­–",
    39: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œåœ¨åœŸåœ°ã€åŠ³åŠ¨åŠ›ã€èµ„æœ¬ã€æŠ€æœ¯ã€æ•°æ®ç­‰è¦ç´ è·å–æ–¹é¢ï¼Œç»™äºˆç‰¹å®šç»è¥è€…ä¼˜æƒ æ”¿ç­–",
    40: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œåœ¨ç¯ä¿æ ‡å‡†ã€æ’æ±¡æƒé™ç­‰æ–¹é¢ç»™äºˆç‰¹å®šç»è¥è€…ç‰¹æ®Šå¾…é‡",
    41: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œå¯¹ç‰¹å®šç»è¥è€…å‡å…ã€ç¼“å¾æˆ–åœå¾è¡Œæ”¿äº‹ä¸šæ€§æ”¶è´¹ã€æ”¿åºœæ€§åŸºé‡‘ã€ä½æˆ¿å…¬ç§¯é‡‘ç­‰",
    42: "å®‰æ’è´¢æ”¿æ”¯å‡ºä¸€èˆ¬ä¸å¾—ä¸ç‰¹å®šç»è¥è€…ç¼´çº³çš„ç¨æ”¶æˆ–éç¨æ”¶å…¥æŒ‚é’©ï¼Œä¸»è¦æŒ‡æ ¹æ®ç‰¹å®šç»è¥è€…ç¼´çº³çš„ç¨æ”¶æˆ–è€…éç¨æ”¶å…¥æƒ…å†µï¼Œé‡‡å–åˆ—æ”¶åˆ—æ”¯æˆ–è€…è¿æ³•è¿è§„é‡‡å–å…ˆå¾åè¿”ã€å³å¾å³é€€ç­‰å½¢å¼ï¼Œå¯¹ç‰¹å®šç»è¥è€…è¿›è¡Œè¿”è¿˜ï¼Œæˆ–è€…ç»™äºˆç‰¹å®šç»è¥è€…è´¢æ”¿å¥–åŠ±æˆ–è¡¥è´´ã€å‡å…åœŸåœ°ç­‰è‡ªç„¶èµ„æºæœ‰å¿ä½¿ç”¨æ”¶å…¥ç­‰ä¼˜æƒ æ”¿ç­–",
    43: "ä¸å¾—è¿æ³•è¿è§„å‡å…æˆ–è€…ç¼“å¾ç‰¹å®šç»è¥è€…åº”å½“ç¼´çº³çš„ç¤¾ä¼šä¿é™©è´¹ç”¨ï¼Œä¸»è¦æŒ‡æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šä¾æ®ï¼Œæ ¹æ®ç»è¥è€…è§„æ¨¡ã€æ‰€æœ‰åˆ¶å½¢å¼ã€ç»„ç»‡å½¢å¼ã€åœ°åŒºç­‰å› ç´ ï¼Œå‡å…æˆ–è€…ç¼“å¾ç‰¹å®šç»è¥è€…éœ€è¦ç¼´çº³çš„åŸºæœ¬å…»è€ä¿é™©è´¹ã€åŸºæœ¬åŒ»ç–—ä¿é™©è´¹ã€å¤±ä¸šä¿é™©è´¹ã€å·¥ä¼¤ä¿é™©è´¹ã€ç”Ÿè‚²ä¿é™©è´¹ç­‰",
    44: "æ²¡æœ‰æ³•å¾‹ã€è¡Œæ”¿æ³•è§„ä¾æ®æˆ–è€…ç»å›½åŠ¡é™¢æ‰¹å‡†ï¼Œè¦æ±‚ç»è¥è€…äº¤çº³å„ç±»ä¿è¯é‡‘",
    45: "é™å®šåªèƒ½ä»¥ç°é‡‘å½¢å¼äº¤çº³æŠ•æ ‡ä¿è¯é‡‘æˆ–å±¥çº¦ä¿è¯é‡‘",
    46: "åœ¨ç»è¥è€…å±¥è¡Œç›¸å…³ç¨‹åºæˆ–è€…å®Œæˆç›¸å…³äº‹é¡¹åï¼Œä¸ä¾æ³•é€€è¿˜ç»è¥è€…äº¤çº³çš„ä¿è¯é‡‘åŠé“¶è¡ŒåŒæœŸå­˜æ¬¾åˆ©æ¯",
    47: "ä¸å¾—å¼ºåˆ¶ç»è¥è€…ä»äº‹ã€Šä¸­åäººæ°‘å…±å’Œå›½åå„æ–­æ³•ã€‹ç¦æ­¢çš„å„æ–­è¡Œä¸ºï¼Œä¸»è¦æŒ‡ä»¥è¡Œæ”¿å‘½ä»¤ã€è¡Œæ”¿æˆæƒã€è¡Œæ”¿æŒ‡å¯¼ç­‰æ–¹å¼æˆ–è€…é€šè¿‡è¡Œä¸šåä¼šå•†ä¼šï¼Œå¼ºåˆ¶ã€ç»„ç»‡æˆ–è€…å¼•å¯¼ç»è¥è€…è¾¾æˆå„æ–­åè®®ã€æ»¥ç”¨å¸‚åœºæ”¯é…åœ°ä½ï¼Œä»¥åŠå®æ–½å…·æœ‰æˆ–è€…å¯èƒ½å…·æœ‰æ’é™¤ã€é™åˆ¶ç«äº‰æ•ˆæœçš„ç»è¥è€…é›†ä¸­ç­‰è¡Œä¸º",
    48: "ä¸å¾—è¿æ³•æŠ«éœ²æˆ–è€…è¿æ³•è¦æ±‚ç»è¥è€…æŠ«éœ²ç”Ÿäº§ç»è¥æ•æ„Ÿä¿¡æ¯ï¼Œä¸ºç»è¥è€…å®æ–½å„æ–­è¡Œä¸ºæä¾›ä¾¿åˆ©æ¡ä»¶ã€‚ç”Ÿäº§ç»è¥æ•æ„Ÿä¿¡æ¯æ˜¯æŒ‡é™¤ä¾æ®æ³•å¾‹ã€è¡Œæ”¿æ³•è§„æˆ–è€…å›½åŠ¡é™¢è§„å®šéœ€è¦å…¬å¼€ä¹‹å¤–ï¼Œç”Ÿäº§ç»è¥è€…æœªä¸»åŠ¨å…¬å¼€ï¼Œé€šè¿‡å…¬å¼€æ¸ é“æ— æ³•é‡‡é›†çš„ç”Ÿäº§ç»è¥æ•°æ®ã€‚ä¸»è¦åŒ…æ‹¬ï¼šæ‹Ÿå®šä»·æ ¼ã€æˆæœ¬ã€è¥ä¸šæ”¶å…¥ã€åˆ©æ¶¦ã€ç”Ÿäº§æ•°é‡ã€é”€å”®æ•°é‡ã€ç”Ÿäº§é”€å”®è®¡åˆ’ã€è¿›å‡ºå£æ•°é‡ã€ç»é”€å•†ä¿¡æ¯ã€ç»ˆç«¯å®¢æˆ·ä¿¡æ¯ç­‰",
    49: "å¯¹å®è¡Œæ”¿åºœæŒ‡å¯¼ä»·çš„å•†å“ã€æœåŠ¡è¿›è¡Œæ”¿åºœå®šä»·",
    50: "å¯¹ä¸å±äºæœ¬çº§æ”¿åºœå®šä»·ç›®å½•èŒƒå›´å†…çš„å•†å“ã€æœåŠ¡åˆ¶å®šæ”¿åºœå®šä»·æˆ–è€…æ”¿åºœæŒ‡å¯¼ä»·",
    51: "è¿åã€Šä¸­åäººæ°‘å…±å’Œå›½ä»·æ ¼æ³•ã€‹ç­‰æ³•å¾‹æ³•è§„é‡‡å–ä»·æ ¼å¹²é¢„æªæ–½",
    52: "åˆ¶å®šå…¬å¸ƒå•†å“å’ŒæœåŠ¡çš„ç»Ÿä¸€æ‰§è¡Œä»·ã€å‚è€ƒä»·",
    53: "è§„å®šå•†å“å’ŒæœåŠ¡çš„æœ€é«˜æˆ–è€…æœ€ä½é™ä»·",
    54: "å¹²é¢„å½±å“å•†å“å’ŒæœåŠ¡ä»·æ ¼æ°´å¹³çš„æ‰‹ç»­è´¹ã€æŠ˜æ‰£æˆ–è€…å…¶ä»–è´¹ç”¨"
}

# æ–‡ä»¶è·¯å¾„ï¼ˆæ¢å¤ç»“æœä¿å­˜è·¯å¾„ï¼‰
input_path = "/home/grp/disk1/FCR-langchain/file"
persist_directory = "/home/grp/disk1/FCR-langchain/vectorstore"
STRUCTURED_CHUNKS_JSON_PATH = "/home/grp/disk1/FCR-langchain/structured_min_chunks_output.json"
VIOLATION_OUTPUT_DIR = "/home/grp/disk1/FCR-langchain/violation_results"  # ç»“æœä¿å­˜ç›®å½•

# æ£€ç´¢/å¹¶è¡Œç›¸å…³é…ç½®
LONG_CHAPTER_WARN_THRESHOLD = 3000
CHAPTER_BM25_THRESHOLD = 0.7
RETRIEVAL_CANDIDATE_SIZE = 20
RETRIEVAL_FINAL_K = 5
MAX_THREAD_WORKERS = 2  # å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆBERT+LLMå„1ä¸ªï¼‰

# æ—¥å¿—é…ç½®ï¼ˆæ˜¾ç¤ºæ›´å¤šç»†èŠ‚ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# æµ‹è¯•æ¨¡å¼ï¼ˆçº¯æ–‡æœ¬è¾“å…¥æµ‹è¯•ï¼‰
TEST_DEMO = True


# ==============================================================================
# 1. BERT åˆ†ç±»å™¨ï¼ˆæ¿€æ´»é¢„æµ‹ï¼Œé™ä½è¿‡æ»¤é˜ˆå€¼ï¼‰
# ==============================================================================
class BertSentenceClassifier:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self._load_model()

    def _load_model(self):
        if not BERT_MODEL_PATH:
            logger.warning("âš ï¸ BERT_MODEL_PATH æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡ŒBERTé¢„æµ‹")
            return
        try:
            # åŠ è½½Tokenizer
            self.tokenizer = BertTokenizer.from_pretrained(BERT_TOKENIZER_PATH)
            # åŠ è½½æ¨¡å‹ï¼ˆCPUï¼‰
            try:
                self.model = torch.jit.load(BERT_MODEL_PATH, map_location="cpu")
            except Exception:
                self.model = torch.load(BERT_MODEL_PATH, map_location="cpu", weights_only=False)
            self.model.eval()
            logger.info(f"âœ… BERTæ¨¡å‹åŠ è½½å®Œæˆï¼ˆè®¾å¤‡ï¼šCPUï¼Œç½®ä¿¡åº¦é˜ˆå€¼ï¼š{BERT_CONFIDENCE_THRESHOLD}ï¼‰")
        except Exception as e:
            logger.error(f"âŒ BERTæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}", exc_info=True)

    def predict(self, sentence: str) -> Tuple[Optional[int], Optional[float]]:
        """æ¿€æ´»é¢„æµ‹ï¼šä¿ç•™label=0ï¼ˆä¸è¿è§„ï¼‰ï¼Œé™ä½ç½®ä¿¡åº¦é˜ˆå€¼"""
        if not self.model or not self.tokenizer:
            logger.warning("âš ï¸ BERTæ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡é¢„æµ‹")
            return None, None
        try:
            # Tokenize
            inputs = self.tokenizer(
                sentence,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=512,
                return_token_type_ids=False
            )
            inputs = {k: v.to("cpu") for k, v in inputs.items()}

            # é¢„æµ‹ï¼ˆç¦ç”¨æ¢¯åº¦ï¼‰
            with torch.no_grad():
                predicted_label, final_probability = self.model(**inputs)  # é€‚é…TwoStageModelè¾“å‡º

            # è¿‡æ»¤æä½ç½®ä¿¡åº¦ï¼ˆä¿ç•™label=0ï¼‰
            if final_probability < BERT_CONFIDENCE_THRESHOLD:
                logger.debug(f"âš ï¸ BERTä½ç½®ä¿¡åº¦è·³è¿‡ï¼ˆå¥å­ï¼š{sentence[:30]}...ï¼Œç½®ä¿¡åº¦ï¼š{final_probability:.3f}ï¼‰")
                return None, None

            logger.debug(
                f"âœ… BERTé¢„æµ‹ç»“æœï¼ˆå¥å­ï¼š{sentence[:30]}...ï¼‰ï¼šlabel={predicted_label}ï¼Œç½®ä¿¡åº¦={final_probability:.3f}")
            return predicted_label, final_probability

        except Exception as e:
            logger.error(f"âŒ BERTé¢„æµ‹å¤±è´¥ï¼ˆå¥å­ï¼š{sentence[:30]}...ï¼‰ï¼š{str(e)}", exc_info=True)
            return None, None


# ==============================================================================
# 2. LLM æ ¸å¿ƒï¼šæ„å›¾è½¬è¯‘+æ®µè½çº§åˆ¤æ–­ï¼ˆæ¢å¤å®Œæ•´åŠŸèƒ½ï¼‰
# ==============================================================================
def simple_llm_call(prompt: str) -> str:
    """LLM APIè°ƒç”¨ï¼ˆæ”¯æŒæ„å›¾è½¬è¯‘+æ®µè½åˆ¤æ–­ï¼‰"""
    try:
        client = LLMClient()
        response = client.simple_chat(prompt)
        logger.debug(f"LLMè°ƒç”¨ç»“æœï¼š{response[:100]}...")
        return response or ""
    except Exception as e:
        logger.error(f"âŒ LLM APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return ""


def intent_translate(user_query: str) -> dict:
    """æ„å›¾è½¬è¯‘ï¼šä¿ç•™is_violation_relatedå­—æ®µï¼Œç”¨äºæ£€ç´¢åˆ¤æ–­"""
    prompt = f"""
                ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}
                ä½œä¸ºâ€œå…¬å¹³ç«äº‰å®¡æŸ¥RAGç³»ç»Ÿâ€æ„å›¾åŠ©æ‰‹ï¼Œä»…è¾“å‡ºJSONï¼ˆæ— å…¶ä»–æ–‡å­—ï¼Œæ— æ³¨é‡Šï¼‰ï¼š
                {{
                  "need_retrieval": "æ˜¯/å¦",  // æ— å…³é—®é¢˜å¡«â€œå¦â€ï¼Œéœ€æŸ¥æ¡æ¬¾å¡«â€œæ˜¯â€
                  "is_violation_related": "æ˜¯/å¦",  // æ¶‰åŠè¿è§„åˆ¤æ–­å¡«â€œæ˜¯â€ï¼Œä»…æŸ¥å®šä¹‰å¡«â€œå¦â€
                  "retrieval_type": "å†…å®¹æ£€ç´¢/è¿è§„å®¡æŸ¥/æ— ",  // æŸ¥æ¡æ¬¾=å†…å®¹æ£€ç´¢ï¼›åˆ¤è¿è§„=è¿è§„å®¡æŸ¥
                  "retrieval_query": "æ£€ç´¢æŒ‡ä»¤ï¼ˆæ— éœ€æ£€ç´¢åˆ™å¡«ç©ºï¼‰",  // ç²¾å‡†æ£€ç´¢è¯­å¥
                  "need_chapter_filter": "æ˜¯/å¦",  // èƒ½æ˜ç¡®ç« èŠ‚ï¼ˆå¦‚â€œå®¡æŸ¥æ ‡å‡†â€ï¼‰å¡«â€œæ˜¯â€
                  "target_chapters": ["ç« èŠ‚å1"]  // æ— éœ€è¿‡æ»¤åˆ™å¡«ç©ºåˆ—è¡¨
                }}
                // è§„åˆ™ï¼š
                // 1. é—®â€œæŸè¡Œä¸ºæ˜¯å¦è¿è§„â€â†’ is_violation_related=æ˜¯
                // 2. é—®â€œå®¡æŸ¥æ ‡å‡†å®šä¹‰â€â†’ is_violation_related=å¦
                """
    prompt = prompt.strip()

    try:
        response = simple_llm_call(prompt)
        parsed = json.loads(response.strip())
        # æ ¼å¼å®¹é”™
        if not isinstance(parsed, dict):
            parsed = {
                "need_retrieval": "æ˜¯",
                "is_violation_related": "å¦",
                "retrieval_type": "å†…å®¹æ£€ç´¢",
                "retrieval_query": user_query,
                "need_chapter_filter": "å¦",
                "target_chapters": []
            }
        # è¡¥å…¨ç¼ºå¤±å­—æ®µ
        intent_dict = {
            "need_retrieval": parsed.get("need_retrieval", "æ˜¯"),
            "is_violation_related": parsed.get("is_violation_related", "å¦"),
            "retrieval_type": parsed.get("retrieval_type", "å†…å®¹æ£€ç´¢"),
            "retrieval_query": parsed.get("retrieval_query", user_query).strip() or user_query,
            "need_chapter_filter": parsed.get("need_chapter_filter", "å¦"),
            "target_chapters": parsed.get("target_chapters", []) if isinstance(parsed.get("target_chapters"),
                                                                               list) else []
        }
        logger.info(f"âœ… æ„å›¾è½¬è¯‘ç»“æœï¼š{json.dumps(intent_dict, ensure_ascii=False)}")
        return intent_dict
    except json.JSONDecodeError as e:
        logger.error(f"âŒ æ„å›¾è¾“å‡ºéJSONï¼ˆå“åº”ï¼š{response[:100]}...ï¼‰ï¼š{str(e)}")
        return {
            "need_retrieval": "æ˜¯",
            "is_violation_related": "å¦",
            "retrieval_type": "å†…å®¹æ£€ç´¢",
            "retrieval_query": user_query,
            "need_chapter_filter": "å¦",
            "target_chapters": []
        }
    except Exception as e:
        logger.error(f"âŒ æ„å›¾è½¬è¯‘å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return {
            "need_retrieval": "æ˜¯",
            "is_violation_related": "å¦",
            "retrieval_type": "å†…å®¹æ£€ç´¢",
            "retrieval_query": user_query,
            "need_chapter_filter": "å¦",
            "target_chapters": []
        }

class CustomBM25Retriever:
    """BM25æ£€ç´¢å™¨ï¼ˆæ”¯æŒç« èŠ‚è¿‡æ»¤ï¼‰"""

    def __init__(self, docs: List[Dict], bm25_model: BM25Okapi):
        self.bm25 = bm25_model
        self.docs = docs
        self._all_chapters = [doc["metadata"]["parent_chapter_title"] for doc in docs]
        self._chapter_bm25 = self._build_chapter_bm25()

    def _build_chapter_bm25(self) -> BM25Okapi:
        """æ„å»ºç« èŠ‚çº§BM25"""
        tokenized_chapters = [list(jieba.cut(chap)) for chap in self._all_chapters]
        return BM25Okapi(tokenized_chapters, k1=1.2, b=0.4)

    def retrieve(self, query: str, top_k: int = 50) -> List[Tuple[float, Document]]:
        """åŸºç¡€BM25æ£€ç´¢"""
        tokenized_query = list(jieba.cut(query))
        scores = self.bm25.get_scores(tokenized_query)
        results = [
            (scores[i], Document(
                page_content=self.docs[i]["page_content"],
                metadata=self.docs[i]["metadata"]
            )) for i in range(len(self.docs)) if scores[i] > 0
        ]
        results.sort(key=lambda x: x[0], reverse=True)
        return results[:top_k]

    def retrieve_with_chapter_filter(self, query: str, target_chapters: List[str], top_k: int = 50) -> List[
        Tuple[float, Document]]:
        """å¸¦ç« èŠ‚è¿‡æ»¤çš„BM25æ£€ç´¢"""
        # åŒ¹é…ç›¸å…³ç« èŠ‚
        tokenized_chap_query = list(jieba.cut(" ".join(target_chapters)))
        chap_scores = self._chapter_bm25.get_scores(tokenized_chap_query)
        relevant_indices = [i for i, score in enumerate(chap_scores) if score > CHAPTER_BM25_THRESHOLD]
        if not relevant_indices:
            logger.warning("âš ï¸ æ— åŒ¹é…ç« èŠ‚ï¼Œæ‰§è¡ŒåŸºç¡€BM25æ£€ç´¢")
            return self.retrieve(query, top_k)

        # è¿‡æ»¤æ–‡æ¡£å¹¶æ£€ç´¢
        filtered_docs = [self.docs[i] for i in relevant_indices]
        filtered_texts = [doc["page_content"] for doc in filtered_docs]
        tokenized_texts = [list(jieba.cut(text)) for text in filtered_texts]
        content_bm25 = BM25Okapi(tokenized_texts)

        tokenized_content_query = list(jieba.cut(query))
        content_scores = content_bm25.get_scores(tokenized_content_query)

        results = [
            (content_scores[i], Document(
                page_content=filtered_docs[i]["page_content"],
                metadata=filtered_docs[i]["metadata"]
            )) for i in range(len(filtered_docs)) if content_scores[i] > 0
        ]
        results.sort(key=lambda x: x[0], reverse=True)
        return results[:top_k]


def build_bm25_retriever(documents: List[Dict]) -> CustomBM25Retriever:
    """æ„å»ºBM25æ£€ç´¢å™¨"""
    texts = [doc["page_content"] for doc in documents if doc["page_content"].strip()]
    if not texts:
        raise ValueError("âŒ æ— æœ‰æ•ˆæ–‡æœ¬å†…å®¹ï¼Œæ— æ³•æ„å»ºBM25æ£€ç´¢å™¨")
    tokenized_corpus = [list(jieba.cut(text)) for text in texts]
    bm25_model = BM25Okapi(tokenized_corpus)
    return CustomBM25Retriever(docs=documents, bm25_model=bm25_model)


def llm_paragraph_judge(
        paragraph: str,
        doc_metadata: Dict[str, Any],
        vectordb: LangChainFAISS,
        bm25_retriever: CustomBM25Retriever
) -> List[Dict[str, Any]]:
    """æ¢å¤LLMæ®µè½çº§åˆ¤æ–­ï¼šç»“åˆæ£€ç´¢ç»“æœåˆ†æè¿è§„"""
    results = []
    try:
        # æ­¥éª¤1ï¼šç”Ÿæˆæ£€ç´¢æ„å›¾
        intent_query = f"åˆ†ææ®µè½æ˜¯å¦å­˜åœ¨å…¬å¹³ç«äº‰è¿è§„ï¼š{paragraph[:200]}..."
        intent_dict = intent_translate(intent_query)
        if intent_dict.get("need_retrieval") == "å¦":
            retrieval_query = intent_query
            logger.debug(f"âš ï¸ LLMåˆ¤æ–­æ— éœ€æ£€ç´¢ï¼Œç›´æ¥åˆ†ææ®µè½ï¼š{paragraph[:50]}...")
        else:
            retrieval_query = intent_dict["retrieval_query"]
            logger.debug(f"âœ… LLMç”Ÿæˆæ£€ç´¢æŒ‡ä»¤ï¼š{retrieval_query}")

        # æ­¥éª¤2ï¼šæ£€ç´¢ç›¸å…³æ¡æ¬¾
        basis_list = hybrid_retrieval(
            vectordb=vectordb,
            bm25_retriever=bm25_retriever,
            query=retrieval_query,
            intent_dict=intent_dict,
            candidate_size=RETRIEVAL_CANDIDATE_SIZE,
            final_k=RETRIEVAL_FINAL_K
        )

        # æ­¥éª¤3ï¼šæ„å»ºLLM Prompt
        basis_text = ""
        for i, (score, doc) in enumerate(basis_list, 1):
            source = doc.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            chapter = doc.metadata.get("parent_chapter_title", "æœªçŸ¥ç« èŠ‚")
            content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
            basis_text += f"æ¡æ¬¾{i}ï¼ˆæ¥æºï¼š{source} - {chapter}ï¼Œç›¸ä¼¼åº¦ï¼š{score:.3f}ï¼‰ï¼š{content}\n"

        llm_prompt = f"""
                    ä½ æ˜¯å…¬å¹³ç«äº‰å®¡æŸ¥ä¸“å®¶ï¼ŒåŸºäºä»¥ä¸‹ä¿¡æ¯åˆ†ææ®µè½è¿è§„æƒ…å†µï¼Œä»…è¾“å‡ºJSONæ•°ç»„ï¼ˆæ— å¤šä½™æ–‡å­—ï¼‰ï¼š
                    1. æå–æ®µè½ä¸­è¿è§„å¥å­ï¼ˆæ— è¿è§„åˆ™è¿”å›ç©ºæ•°ç»„ï¼‰ï¼›
                    2. è¿è§„ç±»å‹åŒ¹é…RULE_MAPPINGï¼ˆå¦‚â€œé™å®šåªèƒ½ä»¥ç°é‡‘å½¢å¼äº¤çº³æŠ•æ ‡ä¿è¯é‡‘â€ï¼‰ï¼›
                    3. ä¾æ®å¿…é¡»å¼•ç”¨å‚è€ƒæ¡æ¬¾ï¼›
                    4. å»ºè®®éœ€å…·ä½“å¯è½åœ°ã€‚

                    ã€å¾…åˆ†ææ®µè½ã€‘
                    {paragraph}

                    ã€å‚è€ƒæ¡æ¬¾ã€‘
                    {basis_text if basis_text else "æ— ç›¸å…³æ¡æ¬¾"}

                    ã€è¾“å‡ºæ ¼å¼ã€‘
                    [
                        {{
                            "violation_sentence": "å®Œæ•´è¿è§„å¥å­",
                            "violation_type": "è¿è§„ç±»å‹åç§°",
                            "confidence": 0.0-1.0,
                            "basis": "å¼•ç”¨æ¡æ¬¾å†…å®¹",
                            "suggestion": "ä¿®æ”¹å»ºè®®",
                            "source": "LLMæ®µè½çº§åˆ†æï¼ˆç«å±±å¼•æ“Arkï¼‰"
                        }}
                    ]
                    """
        llm_prompt = llm_prompt.strip()

        # æ­¥éª¤4ï¼šè°ƒç”¨LLMå¹¶è§£æç»“æœ
        response = simple_llm_call(llm_prompt)
        parsed = json.loads(response.strip())
        llm_results = parsed if isinstance(parsed, list) else [parsed] if isinstance(parsed, dict) else []

        # æ­¥éª¤5ï¼šè¡¥å……å…ƒæ•°æ®
        for res in llm_results:
            results.append({
                "violation_sentence": res.get("violation_sentence", ""),
                "violation_type": res.get("violation_type", "æœªæ˜ç¡®è¿è§„ç±»å‹"),
                "confidence": round(res.get("confidence", 0.0), 3),
                "basis": res.get("basis", "æ— æ˜ç¡®ä¾æ®"),
                "suggestion": res.get("suggestion", "æ— å…·ä½“å»ºè®®"),
                "source": res.get("source", "LLMæ®µè½çº§åˆ†æï¼ˆç«å±±å¼•æ“Arkï¼‰"),
                "file_name": doc_metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶"),
                "file_path": doc_metadata.get("file_path", "æœªçŸ¥è·¯å¾„"),
                "parent_chapter": doc_metadata.get("parent_chapter_title", "æœªçŸ¥ç« èŠ‚"),
                "paragraph_context": paragraph[:100] + "..."
            })
        logger.info(f"âœ… LLMæ®µè½åˆ†æå®Œæˆï¼ˆæ®µè½ï¼š{paragraph[:50]}...ï¼‰ï¼Œå‘ç° {len(results)} æ¡è¿è§„")
    except Exception as e:
        logger.error(f"âŒ LLMæ®µè½åˆ†æå¤±è´¥ï¼ˆæ®µè½ï¼š{paragraph[:50]}...ï¼‰ï¼š{str(e)}", exc_info=True)
    return results


# ==============================================================================
# 3. å¹¶è¡Œå¤„ç†ï¼ˆæ¢å¤BERT+LLMåŒçº¿ç¨‹ï¼Œç¡®ä¿ç­‰å¾…å®Œæˆï¼‰
# ==============================================================================
def parallel_process_document(
        document_chunks: List[Dict],
        bert_classifier: BertSentenceClassifier,
        vectordb: LangChainFAISS,
        bm25_retriever: CustomBM25Retriever
) -> List[Dict[str, Any]]:
    """å¹¶è¡Œæ‰§è¡ŒBERTå¥å­çº§+LLMæ®µè½çº§åˆ†æï¼Œç­‰å¾…åŒçº¿ç¨‹å®Œæˆ"""
    all_bert_results = []
    all_llm_results = []

    # é¢„å¤„ç†è¾“å…¥ï¼šæ‹†åˆ†å¥å­+æ•´ç†å…ƒæ•°æ®
    task_data = []
    for chunk in document_chunks:
        paragraph = chunk.get("page_content", "").strip()
        if not paragraph:
            logger.warning("âš ï¸ ç©ºæ®µè½è·³è¿‡")
            continue
        # æ‹†åˆ†å¥å­ï¼ˆè¿‡æ»¤çŸ­å¥ï¼‰
        sentences = [s.strip() for s in re.split(r'[ã€‚ï¼›;ï¼!ï¼Ÿ?]', paragraph) if len(s.strip()) > 5]
        task_data.append({
            "paragraph": paragraph,
            "sentences": sentences,
            "metadata": chunk.get("metadata", {})
        })
    if not task_data:
        logger.warning("âŒ æ— æœ‰æ•ˆä»»åŠ¡æ•°æ®ï¼Œå¹¶è¡Œå¤„ç†ç»ˆæ­¢")
        return []

    # å®šä¹‰BERTå¤„ç†å‡½æ•°
    def process_bert():
        nonlocal all_bert_results
        logger.info(f"âœ… BERTå¤„ç†å¯åŠ¨ï¼Œå…± {len(task_data)} ä¸ªæ®µè½")
        for data in task_data:
            for sent in data["sentences"]:
                pred_id, confidence = bert_classifier.predict(sent)
                if pred_id is None or confidence is None:
                    continue
                # è§£æè¿è§„ç±»å‹
                violation_type = RULE_MAPPING.get(pred_id, f"æœªçŸ¥ç±»å‹ï¼ˆIDï¼š{pred_id}ï¼‰")
                all_bert_results.append({
                    "violation_sentence": sent + "ã€‚" if not sent.endswith(("ã€‚", "ï¼", "ï¼Ÿ")) else sent,
                    "violation_type": violation_type,
                    "confidence": round(confidence, 3),
                    "basis": f"BERTæ¨¡å‹é¢„æµ‹ï¼ˆç±»åˆ«IDï¼š{pred_id}ï¼Œç½®ä¿¡åº¦ï¼š{confidence:.3f}ï¼‰",
                    "suggestion": f"å‚è€ƒã€Œ{violation_type}ã€ç›¸å…³æ¡æ¬¾è¿›ä¸€æ­¥æ ¸æŸ¥",
                    "source": "BERTå¥å­çº§åˆ†æ",
                    "file_name": data["metadata"].get("file_name", "æœªçŸ¥æ–‡ä»¶"),
                    "file_path": data["metadata"].get("file_path", "æœªçŸ¥è·¯å¾„"),
                    "parent_chapter": data["metadata"].get("parent_chapter_title", "æœªçŸ¥ç« èŠ‚")
                })
        logger.info(f"âœ… BERTå¤„ç†å®Œæˆï¼Œå…±å‘ç° {len(all_bert_results)} æ¡ç»“æœ")

    # å®šä¹‰LLMå¤„ç†å‡½æ•°
    def process_llm():
        nonlocal all_llm_results
        logger.info(f"âœ… LLMå¤„ç†å¯åŠ¨ï¼Œå…± {len(task_data)} ä¸ªæ®µè½")
        for data in task_data:
            llm_res = llm_paragraph_judge(
                paragraph=data["paragraph"],
                doc_metadata=data["metadata"],
                vectordb=vectordb,
                bm25_retriever=bm25_retriever
            )
            all_llm_results.extend(llm_res)
        logger.info(f"âœ… LLMå¤„ç†å®Œæˆï¼Œå…±å‘ç° {len(all_llm_results)} æ¡ç»“æœ")

    # å¯åŠ¨å¹¶è¡Œçº¿ç¨‹ï¼Œç­‰å¾…å®Œæˆ
    with ThreadPoolExecutor(max_workers=MAX_THREAD_WORKERS) as executor:
        futures = [
            executor.submit(process_bert),
            executor.submit(process_llm)
        ]
        # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡éƒ½å®Œæˆï¼ˆé¿å…ä¸»çº¿ç¨‹æå‰é€€å‡ºï¼‰
        for future in futures:
            future.result()

    # ç»“æœå»é‡ï¼ˆæŒ‰â€œå¥å­+è¿è§„ç±»å‹â€å»é‡ï¼‰
    merged_results = []
    seen_keys = set()
    for res in all_bert_results + all_llm_results:
        clean_sent = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', res["violation_sentence"])
        clean_type = res["violation_type"].strip()
        key = (clean_sent, clean_type)
        if key not in seen_keys:
            seen_keys.add(key)
            merged_results.append(res)

    logger.info(f"âœ… å¹¶è¡Œå¤„ç†ç»“æœèåˆå®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(merged_results)} æ¡ä¸é‡å¤ç»“æœ")
    return merged_results


# ==============================================================================
# 4. ç»“æœä¿å­˜ï¼ˆæ¢å¤å®Œæ•´ä¿å­˜åŠŸèƒ½ï¼‰
# ==============================================================================
def save_violation_results(results: List[Dict[str, Any]], output_dir: str = VIOLATION_OUTPUT_DIR):
    """ä¿å­˜è¿è§„ç»“æœä¸ºJSONå’ŒTXTæŠ¥å‘Š"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")

    # ä¿å­˜JSONï¼ˆç»“æ„åŒ–æ•°æ®ï¼‰
    json_path = os.path.join(output_dir, f"violation_results_{timestamp}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… è¿è§„ç»“æœJSONå·²ä¿å­˜ï¼š{json_path}")

    # ä¿å­˜TXTæŠ¥å‘Šï¼ˆäººç±»å¯è¯»ï¼‰
    report_path = os.path.join(output_dir, f"violation_report_{timestamp}.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# å…¬å¹³ç«äº‰å®¡æŸ¥è¿è§„åˆ†ææŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»ç»“æœæ¡æ•°ï¼š{len(results)}\n")
        f.write(f"åˆ†ææ¥æºï¼šBERTå¥å­çº§åˆ†æ + LLMæ®µè½çº§åˆ†æ\n\n")

        # æŒ‰æ–‡ä»¶åˆ†ç»„
        file_groups = {}
        for res in results:
            file_name = res["file_name"]
            if file_name not in file_groups:
                file_groups[file_name] = []
            file_groups[file_name].append(res)

        for file_name, file_res in file_groups.items():
            f.write(f"## æ–‡ä»¶ï¼š{file_name}\n")
            f.write(f"æ–‡ä»¶è·¯å¾„ï¼š{file_res[0]['file_path']}\n")
            f.write(f"ç« èŠ‚ï¼š{file_res[0]['parent_chapter']}\n")
            f.write(f"ç»“æœæ¡æ•°ï¼š{len(file_res)}\n\n")

            for i, res in enumerate(file_res, 1):
                f.write(f"### ç»“æœ {i}\n")
                f.write(f"- è¿è§„å¥å­ï¼š{res['violation_sentence']}\n")
                f.write(f"- è¿è§„ç±»å‹ï¼š{res['violation_type']}\n")
                f.write(f"- ç½®ä¿¡åº¦ï¼š{res['confidence']}\n")
                f.write(f"- ä¾æ®ï¼š{res['basis']}\n")
                f.write(f"- ä¿®æ”¹å»ºè®®ï¼š{res['suggestion']}\n")
                f.write(f"- åˆ†ææ¥æºï¼š{res['source']}\n\n")
    logger.info(f"âœ… è¿è§„æŠ¥å‘ŠTXTå·²ä¿å­˜ï¼š{report_path}")


# ==============================================================================
# 5. æ–‡æ¡£å¤„ç†ï¼šé€‚é…çº¯æ–‡æœ¬è¾“å…¥ï¼ˆè‡ªåŠ¨è¡¥å…ƒæ•°æ®ï¼‰
# ==============================================================================
def extract_policy_type(text: str) -> str:
    """æ”¿ç­–ç±»å‹åˆ¤æ–­ï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰"""
    if "å¥–åŠ±" in text or "è¡¥è´´" in text:
        return "è´¢æ”¿å¥–åŠ±"
    elif "å‡†å…¥" in text or "é—¨æ§›" in text:
        return "å¸‚åœºå‡†å…¥"
    elif "ç«äº‰" in text or "å®¡æŸ¥" in text:
        return "å…¬å¹³ç«äº‰å®¡æŸ¥"
    else:
        return "ç”¨æˆ·è¾“å…¥å†…å®¹"

def get_single_sentence_input(sentence: Optional[str] = None) -> List[Dict]:
    """
    å•å¥æµ‹è¯•ä¸“ç”¨ï¼šåªæ¥æ”¶ä¸€ä¸ªå¥å­ï¼Œå…ƒæ•°æ®æç®€
    sentence: å¯é€‰å‚æ•°ï¼Œä¾¿äºæµ‹è¯•æ—¶ç›´æ¥ä¼ å…¥å¥å­
    """
    if sentence:
        user_input = sentence
    else:
        if TEST_DEMO:
            user_input = "æŠ•æ ‡ä¿è¯é‡‘åº”ä»¥ç°é‡‘å½¢å¼äº¤çº³ï¼Œä¸æ¥å—ä¿å‡½æˆ–å…¶ä»–å½¢å¼ã€‚"
            logger.info(f"ğŸ§ª å•å¥æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨é¢„è®¾è¾“å…¥ï¼š{user_input}")
        else:
            user_input = input("è¯·è¾“å…¥è¦æµ‹è¯•çš„å•ä¸ªå¥å­ï¼š").strip()
            if not user_input:
                logger.error("âŒ è¾“å…¥ä¸ºç©ºï¼Œç¨‹åºç»ˆæ­¢")
                exit()

    # å…ƒæ•°æ®æç®€ï¼šåªä¿ç•™æ¥æºæ ‡è®°
    return [
        {
            "page_content": user_input,
            "metadata": {
                "source": "user_single_sentence"  # æ˜ç¡®æ ‡è®°è¿™æ˜¯å•å¥è¾“å…¥
            }
        }
    ]

def get_document_input() -> List[Dict]:
    """æ¥æ”¶çº¯æ–‡æœ¬è¾“å…¥ï¼Œè‡ªåŠ¨å¡«å……å…ƒæ•°æ®"""
    if TEST_DEMO:
        # æµ‹è¯•æ¨¡å¼ï¼šé¢„è®¾è¾“å…¥
        user_input = "æŠ•æ ‡ä¿è¯é‡‘åº”ä»¥ç°é‡‘å½¢å¼äº¤çº³ï¼Œä¸æ¥å—ä¿å‡½æˆ–å…¶ä»–å½¢å¼ã€‚"
        logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨é¢„è®¾è¾“å…¥ï¼š{user_input}")
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šå‘½ä»¤è¡Œè¾“å…¥
        user_input = input("è¯·è¾“å…¥è¦åˆ†æçš„å¥å­/æ®µè½ï¼š").strip()
        if not user_input:
            logger.error("âŒ è¾“å…¥ä¸ºç©ºï¼Œç¨‹åºç»ˆæ­¢")
            exit()

    # è‡ªåŠ¨è¡¥å…ƒæ•°æ®
    return [
        {
            "page_content": user_input,
            "metadata": {
                "file_name": "ç”¨æˆ·çº¯æ–‡æœ¬è¾“å…¥",
                "file_path": "æ— æœ¬åœ°æ–‡ä»¶è·¯å¾„",
                "parent_chapter_title": "ç”¨æˆ·è¾“å…¥å†…å®¹",
                "policy_type": extract_policy_type(user_input),
                "input_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        }
    ]


def load_and_process_documents(input_dir: str, save_json_path: str) -> List[Dict]:
    """åŠ è½½æœ¬åœ°æ–‡æ¡£ï¼ˆç”¨äºæ„å»ºæ£€ç´¢åº“ï¼‰"""
    from data_process.process_document import PolicyStructurizeAPI  # å»¶è¿Ÿå¯¼å…¥
    logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æœ¬åœ°æ–‡æ¡£ï¼š{input_dir}")
    struct_api = PolicyStructurizeAPI(long_chapter_warn_threshold=LONG_CHAPTER_WARN_THRESHOLD)

    doc_paths = list(Path(input_dir).glob("*.docx"))
    if not doc_paths:
        raise FileNotFoundError(f"âŒ ç›®å½• {input_dir} ä¸‹æ— docxæ–‡æ¡£")

    all_chunks = []
    for doc_path in doc_paths:
        doc_name = doc_path.name
        logger.info(f"ğŸ” å¤„ç†æ–‡æ¡£ï¼š{doc_name}")
        doc_chunks = struct_api.process_document(str(doc_path), doc_name)
        if not doc_chunks:
            logger.warning(f"âš ï¸ æ–‡æ¡£ {doc_name} æ— æœ‰æ•ˆåˆ†å—ï¼Œè·³è¿‡")
            continue
        # è¡¥å……å…ƒæ•°æ®
        for chunk in doc_chunks:
            chunk["metadata"]["file_path"] = str(doc_path)
            chunk["metadata"]["file_name"] = doc_name
            chunk["metadata"]["policy_type"] = extract_policy_type(chunk["page_content"])
        all_chunks.extend(doc_chunks)

    # ä¿å­˜åˆ†å—åˆ°JSON
    with open(save_json_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… æœ¬åœ°æ–‡æ¡£åˆ†å—ä¿å­˜è‡³ï¼š{save_json_path}ï¼ˆå…± {len(all_chunks)} ä¸ªåˆ†å—ï¼‰")
    return all_chunks


def load_structured_chunks_from_json(json_path: str) -> List[Dict]:
    """ä»JSONåŠ è½½æ–‡æ¡£åˆ†å—ï¼ˆæ„å»ºæ£€ç´¢åº“ç”¨ï¼‰"""
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼š{json_path}")
    with open(json_path, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    # å…ƒæ•°æ®å®¹é”™
    for chunk in chunks:
        chunk.setdefault("page_content", "")
        chunk.setdefault("metadata", {})
        chunk["metadata"].setdefault("file_name", "æœªçŸ¥æ–‡ä»¶")
        chunk["metadata"].setdefault("file_path", "æœªçŸ¥è·¯å¾„")
        chunk["metadata"].setdefault("parent_chapter_title", "æœªçŸ¥ç« èŠ‚")
        chunk["metadata"].setdefault("policy_type", "å…¶ä»–æ”¿ç­–")
    return chunks


# ==============================================================================
# 6. æ£€ç´¢æ ¸å¿ƒï¼šFAISS+BM25æ··åˆæ£€ç´¢ï¼ˆä¿ç•™å®Œæ•´åŠŸèƒ½ï¼‰
# ==============================================================================
def build_vector_index(documents: List[Dict], force_recreate=False) -> LangChainFAISS:
    """æ„å»ºFAISSå‘é‡æ£€ç´¢åº“"""
    model_kwargs = {"device": EMBEDDING_DEVICE}
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs=model_kwargs,
        encode_kwargs={"normalize_embeddings": True}
    )

    faiss_index_path = os.path.join(persist_directory, "faiss_index")
    # é‡å»ºç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if force_recreate or not (os.path.exists(faiss_index_path) and os.listdir(faiss_index_path)):
        logger.info(f"ğŸ”„ å¼ºåˆ¶é‡å»ºFAISSç´¢å¼•ï¼ˆembeddingæ¨¡å‹ï¼š{EMBEDDING_MODEL}ï¼‰...")
        if os.path.exists(faiss_index_path):
            import shutil
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç´¢å¼•ï¼š{faiss_index_path}")
            shutil.rmtree(faiss_index_path)
        os.makedirs(faiss_index_path, exist_ok=True)

        # è½¬æ¢ä¸ºLangChain Document
        langchain_docs = [
            Document(page_content=doc["page_content"], metadata=doc["metadata"])
            for doc in documents if doc["page_content"].strip()
        ]
        if not langchain_docs:
            raise ValueError("âŒ æ— æœ‰æ•ˆæ–‡æ¡£å†…å®¹ï¼Œæ— æ³•æ„å»ºå‘é‡ç´¢å¼•")

        vectordb = LangChainFAISS.from_documents(langchain_docs, embeddings)
        vectordb.save_local(faiss_index_path)
        logger.info(f"âœ… æ–°å»ºFAISSç´¢å¼•ï¼š{faiss_index_path}ï¼ˆå…± {len(langchain_docs)} ä¸ªæ–‡æ¡£ï¼‰")
    else:
        vectordb = LangChainFAISS.load_local(
            faiss_index_path,
            embeddings,
            allow_dangerous_deserialization=True
        )
        logger.info(f"âœ… åŠ è½½å·²æœ‰FAISSç´¢å¼•ï¼š{faiss_index_path}")
    return vectordb

def hybrid_retrieval(
        vectordb: LangChainFAISS,
        bm25_retriever: CustomBM25Retriever,
        query: str,
        intent_dict: dict,
        candidate_size: int = 20,
        final_k: int = 10,
        vector_weight: float = 0.5,
        bm25_weight: float = 0.5
) -> List[Tuple[float, Document]]:
    """æ··åˆæ£€ç´¢ï¼ˆFAISS+BM25ï¼‰"""
    candidate_docs = {}

    # 1. FAISSå‘é‡æ£€ç´¢
    try:
        vector_results = vectordb.similarity_search_with_score(query, k=candidate_size)
        for doc, distance in vector_results:
            similarity_score = 1.0 / (1.0 + distance)  # è·ç¦»è½¬ç›¸ä¼¼åº¦
            doc_id = hash(doc.page_content)
            candidate_docs[doc_id] = {
                "doc": doc,
                "vector_score": similarity_score,
                "bm25_score": 0
            }
        logger.debug(f"âœ… FAISSæ£€ç´¢åˆ° {len(vector_results)} æ¡å€™é€‰ç»“æœ")
    except Exception as e:
        logger.error(f"âŒ FAISSæ£€ç´¢å¤±è´¥ï¼š{str(e)}", exc_info=True)

    # 2. BM25æ£€ç´¢ï¼ˆæ”¯æŒç« èŠ‚è¿‡æ»¤ï¼‰
    try:
        need_chapter_filter = intent_dict.get("need_chapter_filter") == "æ˜¯"
        target_chapters = intent_dict.get("target_chapters", [])
        if need_chapter_filter and target_chapters:
            bm25_results = bm25_retriever.retrieve_with_chapter_filter(
                query=query, target_chapters=target_chapters, top_k=candidate_size
            )
        else:
            bm25_results = bm25_retriever.retrieve(query=query, top_k=candidate_size)

        for score, doc in bm25_results:
            doc_id = hash(doc.page_content)
            if doc_id in candidate_docs:
                candidate_docs[doc_id]["bm25_score"] = score
            else:
                if score > 0:
                    candidate_docs[doc_id] = {
                        "doc": doc,
                        "vector_score": 0,
                        "bm25_score": score
                    }
        logger.debug(f"âœ… BM25æ£€ç´¢åˆ° {len(bm25_results)} æ¡å€™é€‰ç»“æœ")
    except Exception as e:
        logger.error(f"âŒ BM25æ£€ç´¢å¤±è´¥ï¼š{str(e)}", exc_info=True)

    # 3. ç»¼åˆå¾—åˆ†æ’åº
    if not candidate_docs:
        logger.warning("âŒ æ··åˆæ£€ç´¢æ— å€™é€‰ç»“æœ")
        return []

    # å¾—åˆ†å½’ä¸€åŒ–
    max_vector = max([d["vector_score"] for d in candidate_docs.values()], default=1)
    max_bm25 = max([d["bm25_score"] for d in candidate_docs.values()], default=1)

    reranked = []
    for doc_item in candidate_docs.values():
        norm_vector = doc_item["vector_score"] / max_vector if max_vector != 0 else 0
        norm_bm25 = doc_item["bm25_score"] / max_bm25 if max_bm25 != 0 else 0
        combined_score = (norm_vector * vector_weight) + (norm_bm25 * bm25_weight)
        reranked.append((combined_score, doc_item["doc"]))

    # å–Top N
    reranked.sort(key=lambda x: x[0], reverse=True)
    final_results = reranked[:final_k]
    logger.info(f"âœ… æ··åˆæ£€ç´¢å®Œæˆï¼Œè¿”å›Top {len(final_results)} ç»“æœ")
    return final_results


# ==============================================================================
# 7. ä¸»å‡½æ•°ï¼ˆå®Œæ•´æµç¨‹ï¼šè¾“å…¥â†’æ„å›¾â†’æ£€ç´¢â†’å¹¶è¡Œâ†’ä¿å­˜ï¼‰
# ==============================================================================
def main():
    FORCE_RECREATE_VECTOR_INDEX = True  # å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼Œè§£å†³ç»´åº¦ä¸åŒ¹é…é—®é¢˜

    try:
        logger.info("=" * 60)
        logger.info("å…¬å¹³ç«äº‰å®¡æŸ¥å…¨æµç¨‹ï¼ˆçº¯æ–‡æœ¬è¾“å…¥+å¹¶è¡Œåˆ†æ+ç»“æœä¿å­˜ï¼‰")
        logger.info("=" * 60)

        # æ­¥éª¤1ï¼šåˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        logger.info("[1/6] åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        # æµ‹è¯•LLMè¿æ¥
        test_response = simple_llm_call("æµ‹è¯•è¿æ¥ï¼šè¿”å›'LLM_OK'")
        if "LLM_OK" not in test_response:
            logger.warning("âš ï¸ LLMè¿æ¥æµ‹è¯•å¼‚å¸¸ï¼Œå¯èƒ½å½±å“åç»­åˆ†æ")
        else:
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        logger.info("[2/6] åˆå§‹åŒ–BERTåˆ†ç±»å™¨...")
        bert_classifier = BertSentenceClassifier()

        # æ­¥éª¤2ï¼šåŠ è½½æ£€ç´¢åº“ï¼ˆæœ¬åœ°æ–‡æ¡£åˆ†å—ï¼‰
        logger.info("[3/6] åŠ è½½æ£€ç´¢åº“æ–‡æ¡£åˆ†å—...")
        try:
            doc_chunks = load_structured_chunks_from_json(STRUCTURED_CHUNKS_JSON_PATH)
        except (FileNotFoundError, ValueError) as e:
            logger.warning(f"âš ï¸ ä»JSONåŠ è½½å¤±è´¥ï¼š{str(e)}ï¼Œå°†é‡æ–°å¤„ç†æœ¬åœ°æ–‡æ¡£")
            doc_chunks = load_and_process_documents(
                input_dir=input_path,
                save_json_path=STRUCTURED_CHUNKS_JSON_PATH
            )
            FORCE_RECREATE_VECTOR_INDEX = True

        if not doc_chunks:
            logger.error("âŒ æ— æœ‰æ•ˆæ–‡æ¡£åˆ†å—ï¼Œç¨‹åºç»ˆæ­¢")
            return
        logger.info(f"âœ… åŠ è½½æ–‡æ¡£åˆ†å— {len(doc_chunks)} ä¸ª")


        # æ­¥éª¤3ï¼šæ„å»ºæ£€ç´¢ç»„ä»¶ï¼ˆFAISS+BM25ï¼‰
        logger.info("[4/6] æ„å»ºæ£€ç´¢ç»„ä»¶...")
        logger.info(f"ğŸ”§ ä½¿ç”¨embeddingæ¨¡å‹ï¼š{EMBEDDING_MODEL}ï¼Œå¼ºåˆ¶é‡å»ºï¼š{FORCE_RECREATE_VECTOR_INDEX}")
        vectordb = build_vector_index(doc_chunks, force_recreate=FORCE_RECREATE_VECTOR_INDEX)
        bm25_retriever = build_bm25_retriever(doc_chunks)
        logger.info("âœ… æ£€ç´¢ç»„ä»¶ï¼ˆFAISS+BM25ï¼‰æ„å»ºå®Œæˆ")

        text = "å‚ä¸è¯„ä¼˜è¯„å¥–ä¼ä¸šéœ€è¦åœ¨æœ¬åœ°è½æˆ·"
        # æ­¥éª¤4ï¼šæ¥æ”¶ç”¨æˆ·çº¯æ–‡æœ¬è¾“å…¥
        logger.info("[5/6] æ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶æ‰§è¡Œå¹¶è¡Œåˆ†æ...")
        user_input_chunks = get_single_sentence_input(text)
        user_text = user_input_chunks[0]["page_content"]

        # æ­¥éª¤5ï¼šæ„å›¾åˆ¤æ–­+æ£€ç´¢ï¼ˆå¯é€‰ï¼Œç”¨äºLLMåˆ†æä¾æ®ï¼‰
        intent_dict = intent_translate(user_text)
        if intent_dict.get("need_retrieval") == "æ˜¯" and intent_dict.get("is_violation_related") == "æ˜¯":
            logger.info(f"ğŸ” æ‰§è¡Œè¿è§„ç›¸å…³æ£€ç´¢ï¼ˆæ£€ç´¢æŒ‡ä»¤ï¼š{intent_dict['retrieval_query']}ï¼‰")
            retrieval_results = hybrid_retrieval(
                vectordb=vectordb,
                bm25_retriever=bm25_retriever,
                query=intent_dict["retrieval_query"],
                intent_dict=intent_dict
            )
            # æ‰“å°æ£€ç´¢ç»“æœï¼ˆå‚è€ƒç”¨ï¼‰
            logger.info("\n" + "=" * 80)
            logger.info(f"ğŸ“Œ æ£€ç´¢ç»“æœå‚è€ƒï¼ˆç”¨æˆ·è¾“å…¥ï¼š{user_text[:50]}...ï¼‰")
            logger.info("=" * 80)
            for i, (score, doc) in enumerate(retrieval_results, 1):
                source = doc.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
                chapter = doc.metadata.get("parent_chapter_title", "æœªçŸ¥ç« èŠ‚")
                content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                logger.info(f"\nã€ç¬¬{i}æ¡ã€‘- ç›¸ä¼¼åº¦ï¼š{score:.3f}")
                logger.info(f"æ¥æºï¼š{source} > {chapter}")
                logger.info(f"å†…å®¹ï¼š{content}")
            logger.info("=" * 80 + "\n")



        # æ­¥éª¤6ï¼šå¹¶è¡Œåˆ†æï¼ˆBERT+LLMï¼‰+ ç»“æœä¿å­˜
        logger.info("ğŸš€ å¯åŠ¨BERT+LLMå¹¶è¡Œåˆ†æ...")
        violation_results = parallel_process_document(
            document_chunks=user_input_chunks,
            bert_classifier=bert_classifier,
            vectordb=vectordb,
            bm25_retriever=bm25_retriever
        )

        # ä¿å­˜ç»“æœ
        if violation_results:
            save_violation_results(violation_results)
            logger.info(f"ğŸ‰ å…¨æµç¨‹å®Œæˆï¼å…±å‘ç° {len(violation_results)} æ¡ç»“æœï¼Œå·²ä¿å­˜è‡³ {VIOLATION_OUTPUT_DIR}")
        else:
            logger.info("ğŸ‰ å…¨æµç¨‹å®Œæˆï¼æœªå‘ç°è¿è§„ç»“æœ")

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{str(e)}", exc_info=True)



if __name__ == "__main__":
    main()